# ğŸ”® Meta-Prompt MCP

> **A Prompting Oracle** â€” An MCP server that bridges Google's official 68-page Prompting Guide 101 with your LLM workflow using RAG.

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

---

## What It Does

Meta-Prompt MCP is a specialized **Model Context Protocol (MCP)** server that acts as an automated "Prompting Oracle." It lets any MCP-compatible host (Claude Desktop, Cursor, etc.) **query Google's Prompting Guide 101** mid-conversation for specific techniques and best practices.

### Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     stdio      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   MCP Host          â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚   Meta-Prompt MCP        â”‚
â”‚   (Claude Desktop,  â”‚                â”‚                          â”‚
â”‚    Cursor, IDEs)    â”‚                â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚                     â”‚                â”‚   â”‚  FastMCP Server   â”‚   â”‚
â”‚                     â”‚                â”‚   â”‚  â€¢ get_google_    â”‚   â”‚
â”‚                     â”‚                â”‚   â”‚    strategy       â”‚   â”‚
â”‚                     â”‚                â”‚   â”‚  â€¢ improve_my_    â”‚   â”‚
â”‚                     â”‚                â”‚   â”‚    prompt         â”‚   â”‚
â”‚                     â”‚                â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                     â”‚                â”‚            â”‚              â”‚
â”‚                     â”‚                â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚                     â”‚                â”‚   â”‚  LlamaIndex RAG  â”‚   â”‚
â”‚                     â”‚                â”‚   â”‚  â€¢ bge-small-en   â”‚   â”‚
â”‚                     â”‚                â”‚   â”‚  â€¢ VectorStore    â”‚   â”‚
â”‚                     â”‚                â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                     â”‚                â”‚            â”‚              â”‚
â”‚                     â”‚                â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚                     â”‚                â”‚   â”‚  ./storage/       â”‚   â”‚
â”‚                     â”‚                â”‚   â”‚  (persisted idx)  â”‚   â”‚
â”‚                     â”‚                â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Features

| Feature | Details |
|---------|---------|
| **`get_google_strategy` tool** | RAG-powered lookup into the 68-page PDF |
| **`improve_my_prompt` prompt** | Template that analyzes and rewrites your prompts using guide techniques |
| **Zero-cost embeddings** | Uses `BAAI/bge-small-en-v1.5` locally â€” no API keys after initial parse |
| **Persistent index** | Vector store cached in `./storage` for near-instant startup |
| **Offline capable** | Runs entirely locally after the one-time index build |

---

## Quick Start

### 1. Install

```bash
# Via uvx (recommended â€” run without installing globally)
uvx meta-prompt-mcp

# Or install via pip
pip install meta-prompt-mcp
```

### 2. Set Up the Data

```bash
# Place Google's Prompting Guide 101 PDF in the data/ directory
cp ~/Downloads/prompting-guide-101.pdf data/
```

### 3. First Run (requires LLAMA_CLOUD_API_KEY)

```bash
# Get a free key at https://cloud.llamaindex.ai/
cp .env.example .env
# Edit .env and add your LLAMA_CLOUD_API_KEY

# The first run parses the PDF and builds the index
meta-prompt-mcp
```

After the first run, `./storage` is created and no API keys are needed.

### 4. Configure Your MCP Host

#### Claude Desktop

Add to your `claude_desktop_config.json`:

```json
{
  "mcpServers": {
    "meta-prompt-mcp": {
      "command": "uvx",
      "args": ["meta-prompt-mcp"]
    }
  }
}
```

#### Cursor

Add to your MCP settings:

```json
{
  "mcpServers": {
    "meta-prompt-mcp": {
      "command": "uvx",
      "args": ["meta-prompt-mcp"]
    }
  }
}
```

---

## Development

```bash
# Clone the repo
git clone <your-repo-url>
cd meta-prompt-mcp

# Install in dev mode
pip install -e ".[dev]"

# Run the server
meta-prompt-mcp
# or
python -m meta_prompt_mcp
```

---

## Environment Variables

| Variable | Required | Description |
|----------|----------|-------------|
| `LLAMA_CLOUD_API_KEY` | First run only | LlamaParse API key for PDF parsing |

---

## Project Structure

```
meta-prompt-mcp/
â”œâ”€â”€ pyproject.toml              # Package config & dependencies
â”œâ”€â”€ README.md
â”œâ”€â”€ .env.example                # Env template
â”œâ”€â”€ data/                       # Place PDF here
â”‚   â””â”€â”€ README.md
â”œâ”€â”€ storage/                    # Auto-generated vector index (gitignored)
â””â”€â”€ src/
    â””â”€â”€ meta_prompt_mcp/
        â”œâ”€â”€ __init__.py
        â”œâ”€â”€ __main__.py         # python -m support
        â”œâ”€â”€ index_manager.py    # LlamaParse + embedding + persistence
        â””â”€â”€ server.py           # FastMCP server with tools & prompts
```

---

## License

MIT
